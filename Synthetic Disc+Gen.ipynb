{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from discriminator.discriminator_wrapper import DiscriminatorWrapper\n",
    "from generator.generator_wrapper import GeneratorWrapper, GeneratorSpec\n",
    "from generator.generator_data import GeneratorData\n",
    "from synthetic.target_data_generator import TargetDataGenerator\n",
    "from synthetic.target_data import TargetData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_sess():\n",
    "    global sess\n",
    "    ruv = set(sess.run(tf.report_uninitialized_variables()))\n",
    "    uv = [v for v in tf.global_variables() if v.name.split(':')[0].encode('ascii') in ruv]\n",
    "    tf.variables_initializer(uv).run()\n",
    "    \n",
    "def reset_sess():\n",
    "    global sess\n",
    "    tf.reset_default_graph()\n",
    "    sess.close()\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "def get_mean_reward(rewards):\n",
    "    np_rewards = np.array(rewards)\n",
    "    rows, columns = np.nonzero(np_rewards)\n",
    "    indices = [i - 1 for i, val in enumerate(rows) if i > 0 and val != rows[i-1] or i == rows.shape[0]-1]\n",
    "    final_rewards = np.zeros((len(indices)))\n",
    "    for i, idx in enumerate(indices):\n",
    "        final_rewards[i] = np_rewards[rows[idx], columns[idx]]\n",
    "    return np.mean(final_rewards)\n",
    "    \n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dim = 32\n",
    "hidden_dim = 32\n",
    "seq_len = 20\n",
    "image_feature_dim = 32\n",
    "data_set_size = 20\n",
    "batch_size = 512\n",
    "\n",
    "embedding_arr = np.random.normal(size=[vocab_size, embedding_dim])\n",
    "\n",
    "tdg = TargetDataGenerator(vocab_size, batch_size, embedding_dim, hidden_dim, seq_len)\n",
    "initialize_sess()\n",
    "\n",
    "raw_data = []\n",
    "for i in range(data_set_size):\n",
    "    raw_data.append(tdg.generate_data(sess))\n",
    "td = TargetData(np.concatenate(raw_data, axis=0), embedding_arr, image_feature_dim, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Loss: 5.94868\n"
     ]
    }
   ],
   "source": [
    "gt_loss = tdg.evaluate(sess, td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_spec = GeneratorSpec(input_dim=None, hidden_dim=hidden_dim * 2, output_dim=vocab_size, rnn_activation=None,\n",
    "                         image_feature_dim=image_feature_dim, n_seq_steps=seq_len-1,\n",
    "                         embedding_init=tf.Variable(embedding_arr, name=\"embedding\", dtype=tf.float32),\n",
    "                         n_baseline_layers=1, baseline_hidden_dim=32,\n",
    "                         mle_learning_rate=1e-2, pg_learning_rate=5e-4,\n",
    "                         baseline_learning_rate=5e-3, batch_size=batch_size, epsilon=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain the Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = GeneratorWrapper(gen_spec, None)\n",
    "initialize_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_loss, mle_cross_entropies, accuracies, discriminator_init_data = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Loss: 10.35077\n",
      "GT Loss: 10.14843\n",
      "GT Loss: 10.08874\n",
      "GT Loss: 10.06451\n",
      "GT Loss: 10.04653\n",
      "GT Loss: 10.00387\n",
      "GT Loss: 9.97735\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    cross_entropy, accuracy = gen.train(sess, td, num_iterations=400, training_type='MLE')\n",
    "    mle_cross_entropies.append(cross_entropy)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    dat, _, __, ___ = gen.test(sess, td, 20)\n",
    "    full_dat = np.ones((dat.shape[0], seq_len)) * tdg.start_token[0]\n",
    "    full_dat[:, 1:] = dat\n",
    "    gd = TargetData(full_dat, embedding_arr, image_feature_dim, vocab_size)\n",
    "    gd.set_mode('MLE').set_batch_size(batch_size)\n",
    "    oracle_loss.append(tdg.evaluate(sess, gd))\n",
    "    discriminator_init_data.append(full_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain the Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bad_data = TargetData(np.concatenate(discriminator_init_data, axis=0), embedding_arr, image_feature_dim, vocab_size)\n",
    "\n",
    "disc = DiscriminatorWrapper(td, bad_data, td, hidden_dim * 2)\n",
    "initialize_sess()\n",
    "\n",
    "for i in range(20):\n",
    "    train_loss, val_loss = disc.pre_train(sess, iter_num=100, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen._discriminator_reward = disc.assign_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prev_sentences = []\n",
    "for cycle in range(10000):\n",
    "    td.shuffle()\n",
    "    caption_sentences, _, img_idxs, r = gen.train(sess, td, 1, training_type='PPO')\n",
    "    prev_sentences.extend(caption_sentences)\n",
    "    if len(prev_sentences) > len(caption_sentences):\n",
    "        idxs = np.random.choice(len(prev_sentences), 2 * len(caption_sentences))\n",
    "        for i in idxs:\n",
    "            caption_sentences.append(prev_sentences[i])\n",
    "        img_idxs = img_idxs * 3\n",
    "    train_losses, val_losses = disc.online_train(sess, 5, np.array(img_idxs), caption_sentences, batch_size=batch_size)\n",
    "    while train_losses[-1] > 1.05:\n",
    "        train_losses, val_losses = disc.online_train(sess, 5, np.array(img_idxs),\n",
    "                                                     caption_sentences, batch_size=batch_size)\n",
    "    \n",
    "    if cycle % data_set_size == 0:\n",
    "        dat, _, __, ___ = gen.test(sess, td, 20)\n",
    "        full_dat = np.ones((dat.shape[0], seq_len)) * tdg.start_token[0]\n",
    "        full_dat[:, 1:] = dat\n",
    "        gd = TargetData(full_dat, embedding_arr, image_feature_dim, vocab_size)\n",
    "        gd.set_mode('MLE').set_batch_size(batch_size)\n",
    "        oracle_loss.append(tdg.evaluate(sess, gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
